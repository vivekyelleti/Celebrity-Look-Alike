{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUNpWkanvljl",
    "outputId": "c8b417b4-3933-486e-88e4-421334a554d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MTCNN\n",
      "  Obtaining dependency information for MTCNN from https://files.pythonhosted.org/packages/09/d1/2a4269e387edb97484157b872fa8a1953b53dcafbe4842a1967f549ac5ea/mtcnn-0.1.1-py3-none-any.whl.metadata\n",
      "  Downloading mtcnn-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: keras>=2.0.0 in /home/durgauser/anaconda3/lib/python3.9/site-packages (from MTCNN) (3.3.2)\n",
      "Collecting opencv-python>=4.1.0 (from MTCNN)\n",
      "  Obtaining dependency information for opencv-python>=4.1.0 from https://files.pythonhosted.org/packages/3f/a4/d2537f47fd7fcfba966bd806e3ec18e7ee1681056d4b0a9c8d983983e4d5/opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: absl-py in /home/durgauser/anaconda3/lib/python3.9/site-packages (from keras>=2.0.0->MTCNN) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/durgauser/anaconda3/lib/python3.9/site-packages (from keras>=2.0.0->MTCNN) (1.24.4)\n",
      "Requirement already satisfied: rich in /home/durgauser/anaconda3/lib/python3.9/site-packages (from keras>=2.0.0->MTCNN) (13.5.2)\n",
      "Requirement already satisfied: namex in /home/durgauser/anaconda3/lib/python3.9/site-packages (from keras>=2.0.0->MTCNN) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/durgauser/anaconda3/lib/python3.9/site-packages (from keras>=2.0.0->MTCNN) (3.11.0)\n",
      "Requirement already satisfied: optree in /home/durgauser/anaconda3/lib/python3.9/site-packages (from keras>=2.0.0->MTCNN) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/durgauser/anaconda3/lib/python3.9/site-packages (from keras>=2.0.0->MTCNN) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/durgauser/anaconda3/lib/python3.9/site-packages (from optree->keras>=2.0.0->MTCNN) (4.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/durgauser/anaconda3/lib/python3.9/site-packages (from rich->keras>=2.0.0->MTCNN) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/durgauser/anaconda3/lib/python3.9/site-packages (from rich->keras>=2.0.0->MTCNN) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/durgauser/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->MTCNN) (0.1.2)\n",
      "Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python, MTCNN\n",
      "Successfully installed MTCNN-0.1.1 opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "!pip install MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Z82TK-2Hvamu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "from mtcnn import MTCNN\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5jeXlK3vfsF",
    "outputId": "5660ad7f-8bb8-4df1-c059-1225b1c00c0a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TripletResNet50' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m detector \u001b[38;5;241m=\u001b[39m MTCNN()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTripletResNet50\u001b[49m(embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the model's state dictionary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./triplet_resnet50_cpu.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TripletResNet50' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the face detector (MTCNN) and feature extractor (ResNet50)\n",
    "detector = MTCNN()\n",
    "\n",
    "# Initialize the model\n",
    "model = TripletResNet50(embedding_dim=128)\n",
    "\n",
    "# Load the model's state dictionary\n",
    "model.load_state_dict(torch.load('./triplet_resnet50_cpu.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wuRT2uWTviLI"
   },
   "outputs": [],
   "source": [
    "# Preprocessing pipeline for images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8eaWnH80vwhZ"
   },
   "outputs": [],
   "source": [
    "# Path to your small dataset of celebrity images\n",
    "celeba_img_path = './celeb_dataset/img_align_celeba/img_align_celeba/'  # Replace with your small dataset directory\n",
    "\n",
    "# Load image file paths from the small dataset\n",
    "celeba_image_files = os.listdir(celeba_img_path)\n",
    "celeba_image_paths = [os.path.join(celeba_img_path, img_file) for img_file in celeba_image_files]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(celeba_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files 202599\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of files\", len(celeba_image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UJ8rX8GjwOVl"
   },
   "outputs": [],
   "source": [
    "# Function to extract face embeddings from an image\n",
    "def extract_face_embedding(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect face in the image\n",
    "    detection = detector.detect_faces(image_rgb)\n",
    "\n",
    "    if len(detection) > 0:\n",
    "        x, y, w, h = detection[0]['box']\n",
    "        face = image_rgb[y:y+h, x:x+w]\n",
    "        face = Image.fromarray(face)\n",
    "        face_tensor = preprocess(face).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = model(face_tensor).numpy()\n",
    "\n",
    "        return embedding\n",
    "    else:\n",
    "        print(f\"No face detected in {image_path}\")\n",
    "        return None\n",
    "\n",
    "# Function to compare the user image against a small set of celebrity images\n",
    "def find_celebrity_look_alike(user_image_path, celeb_image_paths, top_n=3):\n",
    "    user_embedding = extract_face_embedding(user_image_path)\n",
    "\n",
    "    if user_embedding is None:\n",
    "        return \"No face detected in the user image.\"\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    # Compare with each celebrity image\n",
    "    for celeb_path in celeb_image_paths:\n",
    "        celeb_embedding = extract_face_embedding(celeb_path)\n",
    "        if celeb_embedding is not None:\n",
    "            similarity = cosine_similarity(user_embedding, celeb_embedding)[0][0]\n",
    "            similarities.append((celeb_path, similarity))\n",
    "\n",
    "    # Sort celebrities by similarity (highest first)\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top N most similar celebrities\n",
    "    return similarities[:top_n] if similarities else \"No face detected in any celebrity images.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2c940c9550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m celeb_path \u001b[38;5;129;01min\u001b[39;00m celeba_image_paths:\n\u001b[1;32m      5\u001b[0m     celeb_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(celeb_path)\n\u001b[0;32m----> 6\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mceleb_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         celeb_embeddings[celeb_name] \u001b[38;5;241m=\u001b[39m embedding\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mextract_face_embedding\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     face_tensor \u001b[38;5;241m=\u001b[39m preprocess(face)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m         embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(face_tensor)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute celebrity embeddings and save to a file\n",
    "\n",
    "celeb_embeddings = {}\n",
    "for celeb_path in celeba_image_paths:\n",
    "    celeb_name = os.path.basename(celeb_path)\n",
    "    embedding = extract_face_embedding(celeb_path)\n",
    "    if embedding is not None:\n",
    "        celeb_embeddings[celeb_name] = embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'celeb_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Save embeddings to a file (using pickle or numpy)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mceleb_embeddings.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mceleb_embeddings\u001b[49m, f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCelebrity embeddings saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'celeb_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save embeddings to a file (using pickle or numpy)\n",
    "with open('celeb_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(celeb_embeddings, f)\n",
    "\n",
    "print(f\"Celebrity embeddings saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWAr5088wShO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
